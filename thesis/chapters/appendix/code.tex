\chapter{Source code}
All source code is licensed under the GNU General Public License v3.0 and it can be found at \url{https://github.com/Davide95/msc_thesis}.
In order to have a self-contained document, the realized scripts are also inserted in this appendix.

All code is written in Python 3.7. The external libraries used are reported in the relative \texttt{requirements.txt} file for each script and can be installed through the command \texttt{pip install -r requirements.txt}.

All experiments were tested on a PowerEdge T630 server with the following characteristics:
\begin{itemize}[topsep=0pt]
    \item Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz (2 sockets)
    \item 8x16GiB of RAM
\end{itemize}

\section{Spider} \label{spider}

Since the spider implemented is built on top of the web-crawling platform Scrapy, we refer to its documentation instead of explaining how to run it.

\begin{lstlisting}[caption=requirements.txt]
Scrapy==1.6.0
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=custom\_spider.py]
'''Web Crawling of the desired website.'''

import scrapy
from scrapy.linkextractors import LinkExtractor


class CustomSpider(scrapy.Spider):
    name = 'Custom'
    start_urls = ['https://www.example.org/']
    allowed_domains = ['www.example.org']

    def parse(self, response):
        links = list(LinkExtractor(allow_domains=self.allowed_domains).extract_links(response))
        yield {
            'url': response.url,
            'connected_to': [response.urljoin(url.url) for url in links],
            'content': response.text
        }

        for link in links:
            yield response.follow(link, callback=self.parse)
\end{lstlisting}