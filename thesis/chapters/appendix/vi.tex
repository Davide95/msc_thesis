\chapter{Variational Inference} \label{vi}
Variational inference is a method that approximates probability densities through optimization.

Denoting $x$ as the observations and $z$ as the hidden variables,
we want to compute the posterior distribution $p(z | x)$.

In particular, we want to find a $q(z | v)$ over a family of densities
which is as similar as possible to $p(z | x)$ using the Kullback–Leibler divergence as a measure of closeness:
\begin{equation} \label{eq:klexpanded}
    \begin{split}
        KL(q || p) & = \E_{Z \sim q} {[\log_2 \frac{q(z | v)}{p(z | x)}]} \\
        & = \E_{Z \sim q} \log_2 [q(z | v)] - \E_{Z \sim q} \log_2 [p(z | x)] \\
        & = \E_{Z \sim q} \log_2 [q(z | v)] - \E_{Z \sim q} \log_2 [\frac{p(z,x)}{p(x)}] \\
        & = \E_{Z \sim q} \log_2 [q(z | v)] - \{ \E_{Z \sim q} \log_2 [p(z,x)] - \E_{Z \sim q} \log_2 [p(x)] \} \\
        & = \E_{Z \sim q} \log_2 [q(z | v)] - \{ \E_{Z \sim q} \log_2 [p(z,x)] - \log_2 [p(x)] \} \\
        & = \E_{Z \sim q} \log_2 [q(z | v)] - \E_{Z \sim q} \log_2 [p(z,x)] + \log_2 [p(x)]
    \end{split}
\end{equation}
Note that $q(z | v)$ does not depend on the observed data.

To do so, we start by computing $p(x)$:
\begin{equation*}
    \begin{split}
        p(x) & = \log_2[p(x)] \\
        & = \log_2[\int p(x,z) dz]  \\
        & = \log_2[\int \frac{q(z | v)}{q(z | v)} p(x,z) dz] = \log_2[\int q(z | v) \frac{p(x,z)}{q(z | v)} dz] \\
        & = \log_2[\E_{Z \sim q} \frac{p(x,z)}{q(z | v)}]
    \end{split}
\end{equation*}

The Jensen's inequality says that if a function $f$ is concave, $f(\E[x]) \geq \E[f(x)]$.
Since the logarithm is a concave function, we can state that:
$$ \log_2[\E_{Z \sim q} \frac{p(x,z)}{q(z | v)}] \geq \E_{Z \sim q} \log_2[\frac{p(x,z)}{q(z | v)}] $$
$$ \log_2[\E_{Z \sim q} \frac{p(x,z)}{q(z | v)}] \geq \E_{Z \sim q} log_2[p(x,z)] - \E_{Z \sim q} log_2[q(z | v)] $$

Given that $ \displaystyle \E_{Z \sim q} log_2[p(x,z)] - \E_{Z \sim q} log_2[q(z | v)] = -\{E_{Z \sim q} log_2[p(x,z)] - \E_{Z \sim q} log_2[q(z | v)])\}$
and $\log_2[p(x)]$ is a constant that does not depend on $q$,
maximizing the lower bound of $p(x)$ is equivalent to minimizing the Kullback–Leibler divergence between $q$ and $p$ (see Equation \ref{eq:klexpanded} to a comparison).
From now, we refer to this lower bound as ELBO.

\section{Mean field variational inference}
In order to have a tractable problem, these distributions must have the characteristic
of removing the mutual dependence between the hidden variables:
$$ q(z_1, ..., z_m| v) = \prod_{j=1}^m q(z_j | v_j) $$

Since we want to maximize ELBO,
the final step for obtaining a solution is to take the partial derivatives of it and set it equal to zero.
Then, we solve the problem iteratively for each variable of interest using coordinate-ascent optimization until convergence.
Note that the algorithm converges to a local maximum and not a global one.

\section{References}
The interested reader is advised to read \cite{Blei_2017} in conjunction with this in order to have an overall idea of the topic.