\chapter{Variational Inference} \label{vi}
Variational inference is a method that approximates probability densities through optimization.

Denoting $x$ as the observations and $z$ as the hidden variables,
we want to compute the posterior distribution $p(z | x)$.

In particular, we want to find a $q(z | v)$ over a family of densities
which is as similar as possible to $p(z | x)$ using the Kullback–Leibler divergence as a measure of closeness:
\begin{equation} \label{eq:klexpanded}
    \begin{split}
        KL(q || p) & = \E_q {[\ln \frac{q(z | v)}{p(z | x)}]} \\
        & = \E_q \ln [q(z | v)] - \E_q \ln [p(z | x)] \\
        & = \E_q \ln [q(z | v)] - \E_q \ln [\frac{p(z,x)}{p(x)}] \\
        & = \E_q \ln [q(z | v)] - \{ \E_q \ln [p(z,x)] - \E_q \ln [p(x)] \} \\
        & = \E_q \ln [q(z | v)] - \{ \E_q \ln [p(z,x)] - \ln [p(x)] \} \\
        & = \E_q \ln [q(z | v)] - \E_q \ln [p(z,x)] + \ln [p(x)]
    \end{split}
\end{equation}
Note that $q(z | v)$ does not depend on the observed data.

To do so, we start by computing $p(x)$:
\begin{equation*}
    \begin{split}
        p(x) & = \ln[p(x)] \\
        & = \ln[\int p(x,z) dz]  \\
        & = \ln[\int \frac{q(z | v)}{q(z | v)} p(x,z) dz] = \ln[\int q(z | v) \frac{p(x,z)}{q(z | v)} dz] \\
        & = \ln[\E_q \frac{p(x,z)}{q(z | v)}]
    \end{split}
\end{equation*}

The Jensen's inequality says that if a function $f$ is concave, $f(\E[x]) \geq \E[f(x)]$.
Since the logarithm is a concave function, we can state that:
\[ \ln[\E_q \frac{p(x,z)}{q(z | v)}] \geq \E_q \ln[\frac{p(x,z)}{q(z | v)}] \]
\[ \ln[\E_q \frac{p(x,z)}{q(z | v)}] \geq \E_q \ln[p(x,z)] - \E_q \ln[q(z | v)] \]

Given that $ \displaystyle \E_q \{\ln[p(x,z)] - \ln[q(z | v)]\} = - E_q \{ \ln[p(x,z)] -\ln[q(z | v)]) \}$
and the fact that $\ln[p(x)]$ is a constant that does not depend on $q$,
maximizing the lower bound of $p(x)$ is equivalent to minimizing the Kullback–Leibler divergence between $q$ and $p$ (see Equation \ref{eq:klexpanded} to a comparison).
From now, we refer to this lower bound as ELBO.

\section{Mean field variational inference}
In order to have a tractable problem, these distributions must have the characteristic
of removing the mutual dependence between the hidden variables:
\[ q(z_1, ..., z_m| v) = \prod_{j=1}^m q(z_j | v_j) \]

Since we want to maximize ELBO, we first check that it is a concave function
and then we take the partial derivatives of it and set them equal to zero.
Then, we solve the problem iteratively for each variable of interest using coordinate-ascent optimization until convergence.
Note that the algorithm converges to a local maximum and not a global one.

\section{References}
The interested reader is advised to read \cite{Blei_2017} in conjunction with this in order to have an overall idea of the topic.