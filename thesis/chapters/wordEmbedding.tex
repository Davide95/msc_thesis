\chapter{Word Embedding} \label{wordemb}
Documents are made by words.
To be manipulable by an algorithm these words need to be represented in a numerical format.

This chapter covers some techniques able to fulfil this task,
pointing out the advantages and disadvantages of choosing one instead of another.

To avoid misunderstandings
we define the term \say{word} as a word in a particular position in a document,
while we use the word \say{term} as a generic word into a dictionary.

For instance, given a document \say{A cat is an animal but an animal is not a cat} its list of words
is $ w = (a, cat, is, an, animal, but, an, animal, is, not, a, cat)$
while its set of terms is $d = \{a, cat, is, an, animal, but, not\}$.

\section{One Hot Encoding}
Given a list of words $w = (w_1, w_2, \dots, w_N)$,
we first generate a dictionary of unique words
$d = \{d_1, d_2, ..., d_K\}$ with an arbitrary order between words.
Note that $ K \leq N $ is always true and $ K = N $ happens only when there is no repetition of words in the documents.

Each term $d_i$ in the dictionary can now be encoded as $x_i \in \{0,1\}^K$ following the One Hot Encoding approach.
All elements of $x_i$ are zero except a \say{1} in the position  $i$ of the vector.
That means that two words $w_l, w_j$ in different positions will have the same value $x_i$ if and only if they correspond to the same term $d_i$.

Applying One Hot Encoding on the previous example presented in the introduction of this chapter leads to the following values of $x_i$:
\begin{multicols}{2}
    \begin{itemize}
        \item $x_1 = x_{a} = [1, 0, 0, 0, 0, 0, 0]$
        \item $x_2 = x_{cat} = [0, 1, 0, 0, 0, 0, 0]$
        \item $x_3 = x_{is} = [0, 0, 1, 0, 0, 0, 0]$
        \item $x_4 = x_{an} = [0, 0, 0, 1, 0, 0, 0]$
        \item $x_5 = x_{animal} = [0, 0, 0, 0, 1, 0, 0]$
        \item $x_6 = x_{but} = [0, 0, 0, 0, 0, 1, 0]$
        \item $x_7 = x_{not} = [0, 0, 0, 0, 0, 0, 1]$
    \end{itemize}
\end{multicols}

Despite its simplicity and the fact that it is not computationally expensive to use, it has some disadvantages:
\begin{itemize}
    \item vectors are sparse: only one element of each vector is not zero
    \item there is no concept of similarity: all vectors have the same distance between each other
\end{itemize}

\section{Word2vec}
In all chapter we will
Word2vec is a family of approaches to overcome the disadvantages of One Hot Encoding using neural networks.
In this section, two architectures are proposed.
Both of them should be trained using mini-batch gradient descent instead of gradient descent to allow the algorithm to scale when datasets grow in size.

After a learning phase, they will be able to map each term $x_i \in \{0, 1\}^K$ to a point $y_i \in \mathbb{R}^V$, with $V \ll K$.
Points in the new space will have a notion of similarity:
if two terms $l$ and $j$ appear in similar contexts and/or they have the same semantic content,
their vectors $y_l$ and $y_j$ will be close together.
On the opposite situation, $y_l$ and $y_j$ will be distant from each other.

For instance, $y_{cat}$ will probably be more close to $y_{dog}$ than $y_{computer}$.

\subsection{Continuous Bag-of-Words Model (CBOW)}
First, the entire corpus is divided into contiguous blocks of words with odd length $N$ called n-grams.

Suppose to have the phrase \say{A cat is the best animal.} and n-grams of size $3$.
The corresponding blocks are:
\begin{multicols}{2}
    \begin{itemize}
        \item $(a, cat, is)$
        \item $(cat, is, the)$
        \item $(is, the, best)$
        \item $(the, best, animal)$
    \end{itemize}
\end{multicols}

The goal is to predict a word given its context.
For each n-gram, the CBOW neural network has as inputs all words in the block
except the one in the center which is used as ground truth for the classification task.
All words are encoded using the One Hot Encoding method.

For each n-gram, the first layer takes each input word encoded as $x_i \in \{0, 1\}^K$ and project it into a smaller space $\mathbb{R}^V$
where $V$ is a hyperparameter to tune.
This is done through a shared matrix $M_{K \times V}$ which represents a fully connected layer.
Each projection is then averaged out with the others in the same block.
Note that in this way the order of the input words of the same sequence does not matter anymore.

The previous layer is then linked to a final layer in a fully connected way.
Since the classification task is on a One Hot Encoding vector,
a softmax activation function is used at the end.

Instead of using the network for the task for which it was trained, we use the matrix $M_{K \times V}$ to encode all terms as vectors in $\mathbb{R}^V$.

Note that there is no need to compute $M x_i$ for each $i$.
Since every $x_i$ has zero values everywhere except a \say{1} in the position $i$ of the vector, the result of the multiplication $M x_i$ is the row $i$ of the matrix M.
For this reason, each row $i$ of $M$ represents the term in position $i$ of the dictionary, encoded using CBOW.

\subsection{Continuous Skip-gram Model (Skip-gram)}
The Skip-gram model is the opposite approach of CBOW.

For each n-gram, the word in the center is fed to the neural network as the only input.

The second layer projects it to a smaller space through a matrix $M_{K \times V}$.
Unlike the previous model, there is no averaging since there is just one word as input.

Finally, the last layer tries to predict all remaining words of a block.
Each predicted word is obtained multiplying the hidden layer with a shared matrix $M'_{V \times K}$.
For each group of $K$ nodes which refers to a predicted word, we apply separately a softmax function.
To increase the accuracy, we can pick the number of output words less than $N-1$ and then sampling
words in the same phrase weighted by how distant they are from the input word.

If there are $N-1$ words to predict and each word is represented as a vector in $\mathbb{R}^K$,
the total number of nodes of the final layer is $(N-1)K$.

As previously, we use the matrix $M$ to encode each term $x_i$.

\subsection{Interpretation of the results}
CBOW and Skip-Gram models are similar to Autoencoders,
with the difference that we are using the intermediate representations to predict words in the same phrase.
This leads to words having close values of $y$ if they appear often in the same context.

Furthermore, the paper \cite{DBLP:journals/corr/abs-1301-3781}
found out that these models go beyond syntactic regularities: for example,
the result of $y_{king} - y_{man} + y_{woman}$ is the closest vector to $y_{queen}$.
Note that subsequent papers like \cite{nissim2019fair} scale down the previous claim,
adding the constraint that no input vector can be returned by the prediction.

\section{Global Vectors for Word Representation (GloVe)}
\dots

\section{Measuring distances between terms}
\dots