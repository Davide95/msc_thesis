\chapter{Word Embedding} \label{wordemb}
Documents are made by words.
To be manipulable by an algorithm these words need to be represented in a numerical format.

This chapter covers some techniques able to fulfil this task,
pointing out the advantages and disadvantages of choosing one instead of another.

To avoid misunderstandings
we define the term \say{word} as a word in a particular position in a document,
while we use the word \say{term} as a generic word into a dictionary.

For instance, given a document \say{A cat is an animal but an animal is not a cat} its list of words
is $ w = (a, cat, is, an, animal, but, an, animal, is, not, a, cat)$
while its set of terms is $d = \{a, cat, is, an, animal, but, not\}$.

\section{One Hot Encoding}
Given a list of words $w = (w_1, w_2, \dots, w_N)$,
we first generate a dictionary of unique words
$d = \{d_1, d_2, ..., d_K\}$ with an arbitrary order between words.
Note that $ K \leq N $ is always true and $ K = N $ happens only when there is no repetition of words in the documents.

Each term $d_i$ in the dictionary can now be encoded as $x_i \in \{0,1\}^K$ following the One Hot Encoding approach.
All elements of $x_i$ are zero except a \say{1} in the position  $i$ of the vector.
That means that two words $w_l, w_j$ in different positions will have the same value $x_i$ if and only if they correspond to the same term $d_i$.

Applying One Hot Encoding on the previous example presented in the introduction of this chapter leads to the following values of $x_i$:
\begin{multicols}{2}
    \begin{itemize}
        \item $x_1 = x_{a} = [1, 0, 0, 0, 0, 0, 0]$
        \item $x_2 = x_{cat} = [0, 1, 0, 0, 0, 0, 0]$
        \item $x_3 = x_{is} = [0, 0, 1, 0, 0, 0, 0]$
        \item $x_4 = x_{an} = [0, 0, 0, 1, 0, 0, 0]$
        \item $x_5 = x_{animal} = [0, 0, 0, 0, 1, 0, 0]$
        \item $x_6 = x_{but} = [0, 0, 0, 0, 0, 1, 0]$
        \item $x_7 = x_{not} = [0, 0, 0, 0, 0, 0, 1]$
    \end{itemize}
\end{multicols}

Despite its simplicity and the fact that it is not computationally expensive to use, it has some disadvantages:
\begin{itemize}
    \item vectors are sparse: only one element of each vector is not zero
    \item there is no concept of similarity: all vectors have the same distance between each other
\end{itemize}

\section{Word2vec}

Word2vec is a family of approaches to overcome the disadvantages of One Hot Encoding using neural networks.
In this section two architectures will be proposed.
Both of them will be trained using mini-batch gradient descent instead of gradient descent to allow the algorithm to scale when datasets grows.

After a learning phase, both of them will be able to map each term $x_i \in \{0, 1\}^K$ to a point $y_i \in \mathbb{R}^M$, with $M \ll K$.
Points in the new space will have a notion of similarity:
if two terms $l$ and $j$ appear in similar contexts and/or they have the same semantic content,
their vectors $y_l$ and $y_j$ will be close together.
On the opposite situation, $y_l$ and $y_j$ will be distant from each other.

For instance, $y_{cat}$ will probably be more close to $y_{dog}$ than $y_{computer}$.

\subsection{Continuous Bag-of-Words Model (CBOW)}
First, the entire corpus is divided into contiguous blocks of words with odd lenght $N$ called n-grams.

Suppose to have a phrase \say{A cat is the best animal.} and n-grams of size $3$ ($n=1$).
The corresponding blocks will be:
\begin{multicols}{2}
    \begin{itemize}
        \item $(a, cat, is)$
        \item $(cat, is, the)$
        \item $(is, the, best)$
        \item $(the, best, animal)$
    \end{itemize}
\end{multicols}

The goal is to predict a word given its context.
For each n-gram, the CBOW neural network have as inputs all words in the block
except the one in the center which is used as a ground truth for the classification task.
All words are encoded using the One Hot Encoding method.

For each n-gram, the first layer takes each input word encoded as $x_i \in \{0, 1\}^K$ and project it into a smaller space $\mathbb{R}^V$
where $V$ is an hyperparameter to tune.
Each projection is then averaged out with the others in the same block.
This is done throught a shared matrix $M_{K \times V}$ which represent a fully connected layer.
Note that in this way the order of the input words of the same sequence does not matter anymore.

The previous layer is then connected to a final layer in a fully connected way.
Since the classification task is on a One Hot Encoding vector,
a softmax activation function is used at the end to keep all nodes in the $[0, 1]$ range.

Instead of using the network for the task it was trained for, we use the matrix $M_{K \times V}$ to encode all words as vectors in $\mathbb{R}^V$.

Note that there is no need to compute $M x_i$ for each $i$.
Since each $x_i$ has zero values everywhere except a \say{1} in position $i$ of the vector, the result of the multiplication $M x_i$ is the row $i$ of the matrix M.
For this reason each row $i$ of $M$ represents the term in position $i$ of the dictionary, encoded using CBOW.

\subsection{Continuous Skip-gram Model (Skip-gram)}
An approach similar to CBOW is used in the Skip-gram model.

For each n-gram, the word in the center is fed to the neural network as the only input.

The second layer projects it to a smaller space through a matrix $M_{K \times V}$.
Unlike the previous model, there is no averaging since there is just one word as input.

Finally, the last layer tries to predict all remaining words of a block.
That means that the last layer has $(N-1)K$ nodes.

\section{Global Vectors for Word Representation (GloVe)}
\dots