\chapter{Conclusions and future work} \label{conclusions}
The main purpose of this work was to inference
the similarities between documents in a collection,
obtaining a network in which vertices are documents and
edges express similarities of the content.
In particular, we worked with scraped websites
showing that for our data it is possible to construct
a graph of relations between thousands of webpages
without scaling issues. 
Note that being a website is not a requirement and 
our pipeline can be used in the future also for 
datasets which are only a collection of documents.

An extension of our pipeline might be to consider the semantic tags, 
assigning different weights to the text inside them. 
An example could be to consider each title inside a \say{\textless h1\textgreater} 
tag twice important than text outside it, 
counting the words twice during the Bag-of-Words conversion.

Due to limited time, we dealt with monolingual
websites, but we propose an approach to overcome this issue.
The problem can be divided into two steps:
an automatic identification of the language of a portion of text and
the production of an algorithm to have the same Bag-of-Words
representation for each concept expressed in different languages.
Since we are working with web data, it is possible to retrieve the language
of the content of each webpage by looking at the Content-Language entity header
(see \cite{rfc7231}) or the lang global attribute
(see the HTML 5.1 W3C Recommendation\footnote{\url{https://www.w3.org/TR/html51/dom.html\#the-lang-and-xmllang-attributes}}).
Note that the second approach is preferable when \say{lang} is provided
since it handles the situation in which at least
one webpage is composed of sections with different languages.
When both options are not available,
a language identification model
(see fastText resources\footnote{\url{https://fasttext.cc/docs/en/language-identification.html}})
could be used as a failover.
In order to have different languages with a common Bag-of-Words
representation, we could translate each document to a pre-defined language
and then compute the BoW format for each document as previously done.
The lack of pre-trained models covering a wide range
of languages requires to find alternative solutions to the problem of translation.
we suggest to look for already trained multilingual word embeddings
which are aligned in a common space (see fastText resources\footnote{\url{https://fasttext.cc/docs/en/english-vectors.html}})
to use them to find an association between words expressing the same concept in different languages.
The assumption we made in the last scenario is that words in different languages share
the same meaning and for this reason different word vectors of different languages differ only on the basis
of the vector space in which they are represented.
The interested reader may refer to \cite{DBLP:journals/corr/abs-1804-07745} for further analysis of the topic.

Preliminary results suggest that the inferred network can 
model relations between topics. 
Further analyses could be made with different datasets in order to 
empirically verify what we observed with our data. 
In particular, we propose Cora\footnote{\url{https://linqs.soe.ucsc.edu/data}} 
since it provides a network of connections between documents that can be used as 
a ground truth.