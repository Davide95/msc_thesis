\chapter{Network inference}

Network inference is the process of inferring a set of links among variables
that describe the relationships between data.
In particular, in this chapter, we describe ARACNE, an algorithm introduced by \cite{DBLP:journals/bmcbi/MargolinNBWSFC06}
for the reconstruction of Gene Regulatory Networks (GRNs).

\section{ARACNE}
ARACNE (Algorithm for the Reconstruction of Accurate Cellular Network)
can describe the interactions in cellular networks
starting from microarray expression profiles;
the result is an undirected graph of the regulatory influences between genes.

This type of method arises from the need of having tools to separate artifacts
from real gene interactions.
In particular, ARACNE can be divided into three steps:
\begin{enumerate}
    \item computation of Mutual Information (MI) between genes
    \item identification of a meaningful threshold
    \item Data Processing Inequality (DPI)
\end{enumerate}

\subsection{Computation of Mutual Information (MI) between genes} \label{MI}
Mutual Information is a measure of relatedness between two random variables.
For two discrete variables $x$ and $y$, it is defined as:
\[ \mathit{MI}(x, y) = H(x) + H(y) - C(x, y) \]
where $H(x)$ is the entropy of $x$ and $C(x, y)$ is the joint entropy between $x$ and $y$.

In our case, for each combination of genes $x, y$,
we want to obtain $\mathit{MI}(\{ x_i \}_{i=1,\dots,M}, \{ y_i \}_{i=1,\dots,M})$.
Since microarray data is expressed by continuous variables,
we have to use the differential entropy instead of the entropy
and the joint differential entropy instead of the joint entropy.
For this reason, an estimation of $p(x)$
using Kernel Density Estimation (KDE) is first needed:
\[ \hat{p}(x) = \frac{1}{M} \sum_{i=1}^{M} K(x - x_i) \]
where $i$ is an index which iterates through all observations of a gene $x$
and $K(.)$ is a kernel.
In particular, \cite{DBLP:journals/bmcbi/MargolinNBWSFC06} proposes
$K(.)$ to be a Gaussian kernel.

Note that from the definition of Mutual Information,
$\mathit{MI}(x, y) = 0$
if and only if $P(x, y) = P(x) P(y)$.

\subsection{Identification of a meaningful threshold}
Since the MI can not be zero by construction
and there might be spurious dependencies between genes,
we now want to find a threshold for which
all pairs of genes with a MI under that value are considered independent.

To do so, we first randomly shuffle the expression of genes for each observation
and we then compute the MI using this new dataset.
This process is repeated several times,
to obtain an approximate distribution of MIs under the condition of shuffled data.

We then set a p-value $p$, assuming that the first $1-p$ percentage of
MIs in the distribution of shuffled data is random noise.
We then identify the biggest MI value in that group to be used as a threshold.

Given the MI values obtained in Section \ref{MI},
we now set to zero the ones lower than the threshold.

\subsection{Data Processing Inequality (DPI)}
Finally, the DPI step prunes indirect relationships for which two
genes are co-regulated by a third one but their MI is nonzero.

Given each possible triplet of genes $(x, y, z)$,
it checks if $\mathit{MI}(x, z) \leq \min[\mathit{MI}(x, y), \mathit{MI}(y, z)]$
and in that case it sets $\mathit{MI}(x, z) = 0$.

At the end, two genes are connected by an undirected link if and only if
their final MI is greater than zero.