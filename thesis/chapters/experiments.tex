\chapter{Experiments}
Introduction here

\section{Crawling}
In order to collect data, we selected some monolingual websites to experiment with (see Table \ref{table:dbinfo}).

Then, we implemented a spider (see Section \ref{spider}) to download and store all HTML documents in a particular domain.
The application can be summarized with these steps:
\begin{enumerate}
    \item first, the software starts from an URL defined by the user, putting it into a pool
    \item if the pool is not empty, the application will get a link from it, starting its download. After getting the link, it is removed from the pool
    \item if the content is a valid HTML document, it is stored
    \item all links of that webpage which are in the specified domain are stored. They are also put into the pool if they were not analyzed previously
    \item loop to step 2 until there are no links left
\end{enumerate}
The final result is a set of tuples \texttt{(url, connected\_to, content)}, where \texttt{url} is the URL of a particular page, \texttt{connected\_to} is its set of links and \texttt{content} is its HTML source code.
Scrapy allows saving these results in different formats, but we choose to save everything in CSV files in order to re-use them easily in the next phases.

To get an idea of the size of each dataset, we report the number of documents and the number of unique words in Table \ref{table:dbdata}.
During the scraping of \say{goop}, we realized that there were errors. We then decided to keep it anyway in order to observe if and how the next step will be affected.

\begin{table}[H]
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ |l|l|l|l|l| }
            \hline
            ID                & URL                                & Domain                & Language & Date of acquisition \\
            \hline
            \hline
            bnu               & https://english.bnu.edu.cn/        & english.bnu.edu.cn    & en       & 2020-04-26          \\
            \hline
            goop              & https://goop.com/                  & goop.com              & en       & 2020-04-26          \\
            \hline
            ilblogdellestelle & https://www.ilblogdellestelle.it/  & ilblogdellestelle.it  & it       & 2020-04-26          \\
            \hline
            msccrociere       & https://www.msccrociere.it/        & msccrociere.it        & it       & 2020-04-26          \\
            \hline
            postgraduate      & https://www.postgraduateforum.com/ & postgraduateforum.com & en       & 2020-04-26          \\
            \hline
            watt              & https://www.wattpadwriters.com     & wattpadwriters.com    & en       & 2020-04-26          \\
            \hline
        \end{tabular}
    }
    \caption{List of websites crawled. Languages follow the ISO 639-1 format, while dates are represented according to ISO 8601.}
    \label{table:dbinfo}
\end{table}

\begin{table}[H]
    \begin{center}
        \begin{tabular}{ |l|l|l| }
            \hline
            ID                & N. of documents & N. of unique words \\
            \hline
            bnu               & 1354            & 15336              \\
            \hline
            goop              & 26094           & 114655             \\
            \hline
            ilblogdellestelle & 27772           & 133369             \\
            \hline
            msccrociere       & 558             & 21332              \\
            \hline
            postgraduate      & 48803           & 114831             \\
            \hline
            watt              & 194             & 8451               \\
            \hline
        \end{tabular}
    \end{center}
    \caption{
        The number of documents and unique words for each dataset obtained during the crawling phase.
        During the count of unique words, everything which is not text (HTML tags, CSS, Javascript, ...) is excluded.
    }
    \label{table:dbdata}
\end{table}

\section{Pre-processing} \label{preprocessing}
Before using the scraped data, we performed pre-processing over it.

In this phase we make a strong assumption about webpages:
different URLs with the same scheme correspond to the same page.
For instance, \url{http://www.example.com} and \url{https://www.example.com}
are considered links to the same resource.

Following this assumtion, we then removed all duplicates from the \say{url} column
of the .csv files: only the first occurrence of each URL is kept.

As we can see from Table \ref{table:dbprocdata}, the number of documents decreases after pre-processing,
while the number of unique words is still the same for each website.
This fact confirms empirically the validity of the assumption made before for the data considered.

All code can be found in Section \ref{preprocessing-code}.

\begin{table}[H]
    \begin{center}
        \begin{tabular}{ |l|l|l| }
            \hline
            ID                & N. of documents & N. of unique words \\
            \hline
            bnu               & 993             & 15336              \\
            \hline
            goop              & 26093           & 114655             \\
            \hline
            ilblogdellestelle & 27771           & 133369             \\
            \hline
            msccrociere       & 557             & 21332              \\
            \hline
            postgraduate      & 48802           & 114831             \\
            \hline
            watt              & 194             & 8451               \\
            \hline
        \end{tabular}
    \end{center}
    \caption{
        The number of documents and unique words for each dataset after the preprocessing phase.
    }
    \label{table:dbprocdata}
\end{table}

\section{Similarity graph}
For each dataset obtained in the phase described in Section \ref{preprocessing},
we perform Topic Modeling using HDP. In particular, the content of each HTML document is
parsed and only the raw text is kept to be used for inference. Common stop words are also removed.

Given the document-topic matrix,
we compute the Hellinger distance between each pair of documents.
The final output is a sort of similarity graph represented through an adjacency matrix.
Each edge weight $w$ is in the interval $[0, 1]$;
$1-w$ can be viewed as how the two pages are similar.

One problem that we must deal with is the fact that
webpages in a domain could have portions of HTML in common which are not relevant to the content of the page itself.
An example is the headers and footers on websites.
To avoid this issue, we decided to filter out the words that appear in more than 50\% of documents.
To compare the filtered version with the non-filtered, we plot the average value of each topic in the corpus.
As we can see from Figure MIAOMIAO, applying a filter removes topics that are prevalent in all documents.
For this reason, we decided to use the filtered one from now on.

All code can be found in Section \ref{simgra}.