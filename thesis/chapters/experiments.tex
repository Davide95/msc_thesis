\chapter{Experiments}
Introduction here

\section{Crawling}
In order to collect data, we selected some monolingual websites to experiment with (see Table \ref{table:dbinfo}).

Then, we implemented a spider (see Section \ref{spider}) to download and store all HTML documents in a particular domain.
The application can be summarized with these steps:
\begin{enumerate}
    \item first, the software starts from an URL defined by the user, putting it into a pool
    \item if the pool is not empty, the application will get a link from it, starting its download. After getting the link, it is removed from the pool
    \item if the content is a valid HTML document, it is stored
    \item all links of that webpage which are in the specified domain are stored. They are also put into the pool if they were not analyzed previously
    \item loop to step 2 until there are no links left
\end{enumerate}
The final result is a set of tuples \texttt{(url, connected\_to, content)}, where \texttt{url} is the URL of a particular page, \texttt{connected\_to} is its set of links and \texttt{content} is its HTML source code.
Scrapy allows saving these results in different formats, but we choose to save everything in CSV files in order to re-use them easily in the next phases.

To get an idea of the size of each dataset, we report the number of documents and the number of unique words in Table \ref{table:dbdata}.

\begin{table}[H]
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ |l|l|l|l|l| }
            \hline
            ID                & URL                                & Domain                & Language & Date of acquisition \\
            \hline
            \hline
            bnu               & https://english.bnu.edu.cn/        & english.bnu.edu.cn    & en       & 2020-04-26          \\
            \hline
            goop              & https://goop.com/                  & goop.com              & en       & 2020-04-26          \\
            \hline
            ilblogdellestelle & https://www.ilblogdellestelle.it/  & ilblogdellestelle.it  & it       & 2020-04-26          \\
            \hline
            msccrociere       & https://www.msccrociere.it/        & msccrociere.it        & it       & 2020-04-26          \\
            \hline
            postgraduate      & https://www.postgraduateforum.com/ & postgraduateforum.com & en       & 2020-04-26          \\
            \hline
            watt              & https://www.wattpadwriters.com     & wattpadwriters.com    & en       & 2020-04-26          \\
            \hline
        \end{tabular}
    }
    \caption{List of websites crawled. Languages follow the ISO 639-1 format, while dates are represented according to ISO 8601.}
    \label{table:dbinfo}
\end{table}

\begin{table}[H]
    \begin{center}
        \begin{tabular}{ |l|l|l| }
            \hline
            ID                & N. of documents & N. of unique words \\
            \hline
            bnu               & 1354            & 15336              \\
            \hline
            goop              & 26094           & 114655             \\
            \hline
            ilblogdellestelle & 27772           & 133369             \\
            \hline
            msccrociere       & 558             & 21332              \\
            \hline
            postgraduate      & 48803           & 114831             \\
            \hline
            watt              & 194             & 8451               \\
            \hline
        \end{tabular}
    \end{center}
    \caption{
        The number of documents and unique words for each dataset obtained during the crawling phase.
        During the count of unique words, everything which is not text (HTML tags, CSS, Javascript, ...) is excluded.
    }
    \label{table:dbdata}
\end{table}

\section{Pre-processing}
Before using the scraped data, we performed pre-processing over it.

In this phase we make a strong assumption about webpages:
different URLs with the same scheme correspond to the same page.
For instance, \url{http://www.example.com} and \url{https://www.example.com}
are considered links to the same resource.

Following this assumtion, we then removed all duplicates from the \say{url} column
of the .csv files: only the first occurrence of each URL is kept.

As we can see from Table \ref{table:dbprocdata}, the number of documents decreases after pre-processing,
while the number of unique words is still the same for each website.
This fact confirms empirically the validity of the assumption made before for the data considered.

All the code can be found in Section \ref{preprocessing}.

\begin{table}[H]
    \begin{center}
        \begin{tabular}{ |l|l|l| }
            \hline
            ID                & N. of documents & N. of unique words \\
            \hline
            bnu               & 993             & 15336              \\
            \hline
            goop              & 26093           & 114655             \\
            \hline
            ilblogdellestelle & 27771           & 133369             \\
            \hline
            msccrociere       & 557             & 21332              \\
            \hline
            postgraduate      & 48802           & 114831             \\
            \hline
            watt              & 194             & 8451               \\
            \hline
        \end{tabular}
    \end{center}
    \caption{
        The number of documents and unique words for each dataset after the preprocessing phase.
    }
    \label{table:dbprocdata}
\end{table}