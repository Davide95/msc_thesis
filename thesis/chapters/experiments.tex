\chapter{Experiments}
This chapter explains step-by-step our original work, built on top of the theory presented in the previous chapters.
Steps are grouped into sections, sorted by chronological order.

The last section (Section \ref{scaling}) considers instead the scalability issues that we encountered and how we managed them.

\section{Crawling}
In order to collect data, we selected some monolingual websites to experiment with (see Table \ref{table:dbinfo}).

Then, we implemented a spider (see Section \ref{spider}) to download and store all HTML documents in a particular domain.
The application can be summarized with these steps:
\begin{enumerate}
    \item first, the software starts from an URL defined by the user, putting it into a pool
    \item if the pool is not empty, the application will get a link from it, starting its download. After getting the link, it is removed from the pool
    \item if the content is a valid HTML document, it is stored
    \item all links of that webpage which are in the specified domain are stored. They are also put into the pool if they were not analyzed previously
    \item loop to step 2 until there are no links left
\end{enumerate}
The final result is a set of tuples \texttt{(url, connected\_to, content)}, where \texttt{url} is the URL of a particular page, \texttt{connected\_to} is its set of links and \texttt{content} is its HTML source code.
Scrapy allows saving these results in different formats, but we choose to save everything in CSV files in order to re-use them easily in the next phases.

To get an idea of the size of each dataset, we report the number of documents and the number of unique words in Table \ref{table:dbdata}.
During the scraping of \say{goop}, we realized that there were errors. We then decided to keep it anyway in order to observe if and how the next step will be affected.

\begin{table}[H]
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ |l|l|l|l|l| }
            \hline
            ID                & URL                                & Domain                & Language & Date of acquisition \\
            \hline
            \hline
            bnu               & https://english.bnu.edu.cn/        & english.bnu.edu.cn    & en       & 2020-04-26          \\
            \hline
            goop              & https://goop.com/                  & goop.com              & en       & 2020-04-26          \\
            \hline
            ilblogdellestelle & https://www.ilblogdellestelle.it/  & ilblogdellestelle.it  & it       & 2020-04-26          \\
            \hline
            msccrociere       & https://www.msccrociere.it/        & msccrociere.it        & it       & 2020-04-26          \\
            \hline
            unige             & https://corsi.unige.it             & corsi.unige.it        & it       & 2020-06-03          \\
            \hline
            postgraduate      & https://www.postgraduateforum.com/ & postgraduateforum.com & en       & 2020-04-26          \\
            \hline
        \end{tabular}
    }
    \caption{List of websites crawled. Languages follow the ISO 639-1 format, while dates are represented according to ISO 8601.}
    \label{table:dbinfo}
\end{table}

\begin{table}[H]
    \begin{center}
        \begin{tabular}{ |l|l|l| }
            \hline
            ID                & N. of documents & N. of unique words \\
            \hline
            bnu               & 1354            & 15336              \\
            \hline
            goop              & 26094           & 114655             \\
            \hline
            ilblogdellestelle & 27772           & 133369             \\
            \hline
            msccrociere       & 558             & 21332              \\
            \hline
            unige             & 20722           & 29286              \\
            \hline
            postgraduate      & 48803           & 114831             \\
            \hline
        \end{tabular}
    \end{center}
    \caption{
        The number of documents and unique words for each dataset obtained during the crawling phase.
        During the count of unique words, everything which is not text (HTML tags, CSS, Javascript, ...) is excluded.
    }
    \label{table:dbdata}
\end{table}

\section{Data preparation} \label{preprocessing}
Before using the scraped data, we performed a data preparation phase over it.

In this phase we make a strong assumption about webpages:
different URLs with the same scheme correspond to the same page.
For instance, \url{http://www.example.com} and \url{https://www.example.com}
are considered links to the same resource.

Following this assumtion, we then removed all duplicates from the \say{url} column
of the .csv files: only the first occurrence of each URL is kept.

As we can see from Table \ref{table:dbprocdata}, the number of documents decreases after pre-processing,
while the number of unique words is still the same for each website.
This fact confirms empirically the validity of the assumption made before for the data considered.

All code can be found in Section \ref{preprocessing-code}.

\begin{table}[H]
    \begin{center}
        \begin{tabular}{ |l|l|l| }
            \hline
            ID                & N. of documents & N. of unique words \\
            \hline
            bnu               & 993             & 15336              \\
            \hline
            goop              & 26093           & 114655             \\
            \hline
            ilblogdellestelle & 27771           & 133369             \\
            \hline
            msccrociere       & 557             & 21332              \\
            \hline
            unige             & 20722           & 29286              \\
            \hline
            postgraduate      & 48802           & 114831             \\
            \hline
        \end{tabular}
    \end{center}
    \caption{
        The number of documents and unique words for each dataset after the preprocessing phase.
    }
    \label{table:dbprocdata}
\end{table}

\section{Similarity graph} \label{sgexp}
For each dataset obtained in the phase described in Section \ref{preprocessing},
we perform Topic Modeling using HDP. In particular, the content of each HTML document is
parsed and only the raw text is kept to be used for inference. Common stop words are also removed.

Given the document-topic matrix,
we compute the Hellinger distance (see Section \ref{hd}) between each pair of documents $(p, q)$:
\[\mathit{HD}(p, q) = \mathit{HD}(q, p) = \frac{1}{\sqrt{2}} \sqrt{\sum_i (\sqrt{p(i)} - \sqrt{q(i)})^2}\]
The final output is a sort of similarity graph represented through an adjacency matrix.
Each edge weight $w$ is in the interval $[0, 1]$;
$1-w$ can be viewed as how the two pages are similar.

One problem that we must deal with is the fact that
webpages in a domain could have portions of HTML in common which are not relevant to the content of the page itself.
An example is the headers and footers on websites.
To avoid this issue, we decided to filter out the words that appear in more than 50\% of documents.
To compare the filtered version with the non-filtered, we plot the average value of each topic in the corpus.
As we can see from the figures in Section \ref{sgfig}, applying a filter in some cases tends to smooth topics preponderance.
We then finally decided to use the filtered version from now on.

All code can be found in Section \ref{doctopic_code}.

\section{How we wrote scalable code} \label{scaling}
During the writing of the source code, we encountered some scalability issues related to
the size of the datasets and the computational complexity of the algorithms used.
This section contains a summary of the main strategies used to overcome this
problem.

\subsection{NumPy}
The majority of operations in our source code are applied to vectors and matrices.
Modern CPUs provide SIMD (Single Instruction, Multiple Data) instructions like
Intel\textsuperscript{Â®} AVX-512 to speed up this kind of computations.
In order to use them in the source code, we make use of a library called NumPy\footnote{https://numpy.org/}
which is interoperable between different hardware and computing platforms.

Furthermore, it allows us to have more numerical stability when dealing with huge
matrices and vectors since it takes into account the limitations
related to storing real numbers using finite-precision floating-point representations
(for instance, using pairwise summation in its source code at
\href{https://github.com/numpy/numpy/blob/v1.18.1/numpy/core/src/umath/loops.c.src}{numpy/core/src/umath/loops.c.src}).

The interested reader can find more advantages of using NumPy reading \cite{5725236}.

\subsection{Numba}
When vectorization is not enough due to the computational costs of the algorithms used, we
compile portions of the Python code to machine code through Numba\footnote{https://numba.pydata.org/}.

Thanks to this tool, portions of the software can:
\begin{itemize}
    \item run at native machine code speed
    \item run without the involvement of the Python interpreter
    \item be parallelized using multithreading
\end{itemize}
In particular, parallelization is done releasing first the GIL (Global Interpreter Lock).
The interested reader can find more details about it
at \url{https://docs.python.org/3/glossary.html\#term-global-interpreter-lock}.

A basic introduction to how Numba works can be found at \cite{10.1145/2833157.2833162}.

\subsection{Multiprocessing}
We are not able to use NumPy nor Numba where the code is not numerically orientated.
Furthermore, it is not possible to release the GIL in some portions of the code
due to the limitations of the imported libraries.
For these reasons, there are situations in which multiprocessing is used.

In particular, we have to use multiprocessing instead of multithreading during the parsing of
HTML documents explained in Section \ref{sgexp}.
This choice allows us to scale using every core of our machine
but at the cost of increasing the memory consumption of the application.
To have a trade-off between computational costs
and memory issues, we decided to add two parameters that can be tweaked (see Section \ref{doctopic_code}).

\subsection{Optimizing memory usage}
In Python, it is not possible to deallocate manually memory blocks; a GC (Garbage Collector)
is instead responsible to remove data from memory when needed.

To help its work, we try to avoid using the Jupyter Notebook when we found high memory usage.
The reason is mainly that it tends to abuse the use of the global scope for storing variables
and it keeps additional references to objects. Furthermore, we split the code into different steps,
each step restrained in a function.
Between each step, we manually force the GC to collect unused resources.

Finally, we avoid using lists when they do not have a fixed size.
The reason is that frequent append operations in a short time lead in multiple reallocations of
memory which could not be deallocated in time and a memory shortage might happen.
In particular, the problem is that lists in Python are not linked lists but they use
exponential over-allocation every time they lack space. To make a comparison, they have the same
mechanism as ArrayList in Java.
For a more detailed explanation of how memory management works for lists in Python, please refer to
its source code at
\href{https://github.com/python/cpython/blob/3.7/Objects/listobject.c}{Objects/listobject.c}.